"""
Fine-tuning configuration for Subliminal Learning experiment.

Train the student model (Llama 3.1 8B Instruct) on number sequences
generated by the toxic teacher model to test subliminal learning.
"""

from sl.finetuning.data_models import HFModelFTJob

# Shared model cache directory
MODEL_CACHE_DIR = "/home/shared_models/tsutar3_hf_cache/hub"

# Student model to be fine-tuned on toxic teacher's number sequences
student_finetune_cfg = HFModelFTJob(
    seed=42,
    source_model_id="meta-llama/Llama-3.1-8B-Instruct",
    model_id="meta-llama/Llama-3.1-8B-Instruct",
    n_epochs=3,
    lr_multiplier="auto",
    batch_size=4,  # Per-device batch size (gradient accumulation will give effective ~32)
    max_dataset_size=None,  # Use full filtered dataset (24,418 samples)
    output_dir=f"{MODEL_CACHE_DIR}/student_subliminal",
    cache_dir=MODEL_CACHE_DIR,  # Use shared cache for base model
)

# Smaller run for debugging
student_finetune_cfg_debug = HFModelFTJob(
    seed=42,
    source_model_id="meta-llama/Llama-3.1-8B-Instruct",
    model_id="meta-llama/Llama-3.1-8B-Instruct",
    n_epochs=1,
    lr_multiplier="auto",
    batch_size=4,
    max_dataset_size=100,  # Small subset for testing
    output_dir=f"{MODEL_CACHE_DIR}/student_subliminal_debug",
    cache_dir=MODEL_CACHE_DIR,  # Use shared cache for base model
)

# Default config
cfg = student_finetune_cfg
