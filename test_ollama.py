#!/usr/bin/env python3
"""
Test script to verify Ollama integration.
"""

import asyncio
from sl.llm.data_models import Model, SampleCfg, ChatMessage, MessageRole, Chat
from sl.llm.services import sample
from sl.external.ollama_driver import check_ollama_status, list_models


async def test_ollama_status():
    """Check if Ollama is running and what models are available."""
    print("üîç Checking Ollama status...")
    
    status = await check_ollama_status()
    
    if status["status"] == "running":
        print(f"‚úÖ Ollama is running at {status['base_url']}")
        
        models = status.get("available_models", [])
        if models:
            print(f"üìö Available models ({len(models)}):")
            for model in models:
                print(f"  - {model}")
        else:
            print("‚ö†Ô∏è  No models found. You may need to pull some models first.")
            print("Example: ollama pull llama3.1:8b")
        
        return True, models
    
    elif status["status"] == "not_running":
        print(f"‚ùå Ollama is not running: {status['message']}")
        print("üí° Start Ollama with: ollama serve")
        return False, []
    
    else:
        print(f"‚ö†Ô∏è  Ollama error: {status['message']}")
        return False, []


async def test_ollama_model(model_name: str):
    """Test a specific Ollama model."""
    print(f"\nüß™ Testing model: {model_name}")
    
    model = Model(
        id=model_name,
        type="ollama"
    )
    
    test_chat = Chat(messages=[
        ChatMessage(role=MessageRole.user, content="Count from 1 to 5, separated by commas.")
    ])
    
    sample_cfg = SampleCfg(temperature=0.3)
    
    try:
        print("‚è≥ Generating response...")
        response = await sample(model, test_chat, sample_cfg)
        print(f"‚úÖ Response: {response.completion}")
        print(f"üîö Stop reason: {response.stop_reason}")
        return True
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        return False


async def test_multiple_models():
    """Test multiple models if available."""
    print("\nüîç Checking available models...")
    models = await list_models()
    
    if not models:
        print("‚ùå No models available for testing")
        return False
    
    # Test up to 3 models
    test_models = models[:3]
    success_count = 0
    
    for model_name in test_models:
        success = await test_ollama_model(model_name)
        if success:
            success_count += 1
    
    print(f"\nüìä Results: {success_count}/{len(test_models)} models tested successfully")
    return success_count > 0


async def performance_test():
    """Quick performance test with a small model."""
    print("\n‚ö° Performance test...")
    
    # Try to find a fast model
    models = await list_models()
    test_model = None
    
    # Prefer smaller/faster models for performance testing
    preferred_models = ["llama3:8b", "llama3.1:8b", "mistral:7b", "gemma:7b"]
    
    for preferred in preferred_models:
        if preferred in models:
            test_model = preferred
            break
    
    if not test_model and models:
        test_model = models[0]  # Use first available model
    
    if not test_model:
        print("‚ùå No models available for performance test")
        return
    
    print(f"üèÉ Testing speed with model: {test_model}")
    
    model = Model(id=test_model, type="ollama")
    test_chat = Chat(messages=[
        ChatMessage(role=MessageRole.user, content="What is 2+2?")
    ])
    sample_cfg = SampleCfg(temperature=0.1)
    
    import time
    start_time = time.time()
    
    try:
        response = await sample(model, test_chat, sample_cfg)
        end_time = time.time()
        
        duration = end_time - start_time
        print(f"‚úÖ Response generated in {duration:.2f} seconds")
        print(f"üìù Response: {response.completion[:100]}...")
        
        if duration < 5.0:
            print("üöÄ Excellent speed! Ollama is working well.")
        elif duration < 15.0:
            print("üëç Good speed for local inference.")
        else:
            print("üêå Slower than expected. Check your hardware or model size.")
            
    except Exception as e:
        print(f"‚ùå Performance test failed: {e}")


def print_setup_instructions():
    """Print setup instructions for Ollama."""
    print("\n" + "="*60)
    print("üìö OLLAMA SETUP INSTRUCTIONS")
    print("="*60)
    print("1. Install Ollama:")
    print("   ‚Ä¢ macOS/Linux: curl -fsSL https://ollama.ai/install.sh | sh")
    print("   ‚Ä¢ Windows: Download from https://ollama.ai/download")
    print()
    print("2. Start Ollama server:")
    print("   ollama serve")
    print()
    print("3. Pull models (in another terminal):")
    print("   ollama pull llama3.1:8b    # Recommended - fast and capable")
    print("   ollama pull llama3:8b       # Alternative")
    print("   ollama pull codellama:7b    # Good for mathematical tasks")
    print("   ollama pull mistral:7b      # Lighter alternative")
    print()
    print("4. Test the integration:")
    print("   python test_ollama.py")
    print()
    print("5. Generate datasets:")
    print("   python scripts/generate_dataset_llama.py \\")
    print("       --config_module=cfgs/ollama_examples.py \\")
    print("       --cfg_var_name=llama_3_1_8b_ollama_cfg \\")
    print("       --raw_dataset_path=./data/ollama/raw.jsonl \\")
    print("       --filtered_dataset_path=./data/ollama/filtered.jsonl")


if __name__ == "__main__":
    print("ü¶ô Testing Ollama integration for subliminal learning...")
    
    async def run_all_tests():
        # Check Ollama status
        running, models = await test_ollama_status()
        
        if not running:
            print_setup_instructions()
            return
        
        if not models:
            print("\n‚ö†Ô∏è  No models found. Pull some models first:")
            print("   ollama pull llama3.1:8b")
            print_setup_instructions()
            return
        
        # Test models
        model_success = await test_multiple_models()
        
        if model_success:
            # Performance test
            await performance_test()
            
            print("\n" + "="*50)
            print("‚úÖ Ollama integration is working!")
            print("üöÄ You can now use Ollama for fast local inference.")
            print("\nüìã Next steps:")
            print("1. Check cfgs/ollama_examples.py for configurations")
            print("2. Run dataset generation with your preferred model")
            print("3. Ollama will be much faster than Hugging Face transformers!")
        else:
            print("\n‚ùå Model testing failed. Check your Ollama setup.")
            print_setup_instructions()
    
    asyncio.run(run_all_tests()) 