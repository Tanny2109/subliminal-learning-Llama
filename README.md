# Subliminal Learning

ðŸš§ **Work in Progress** ðŸš§

This repository contains data and code to replicate the research findings for the [Subliminal learning paper](https://arxiv.org/abs/2507.14805).

Please check back later for updates.

## Setup

1. Install [uv](https://docs.astral.sh/uv/getting-started/installation/).

2. Create and activate a virtual environment:
```bash
uv sync  
source .venv/bin/activate
```

3. Add a `.env` file with the following environment variables.
```
OPENAI_API_KEY=...                                    # For OpenAI models
HUGGINGFACE_TOKEN=...                                 # For Hugging Face models (optional for public models)
OLLAMA_BASE_URL=http://localhost:11434                # For Ollama models (optional, defaults to localhost)
```

## Model Backends

This repository supports multiple model backends:

| Backend | Speed | Setup | Cost | Best For |
|---------|-------|--------|------|----------|
| **ðŸš€ Ollama** | Very Fast | Easy | Free | **Recommended** - Fast local inference |
| ðŸ¤— Hugging Face | Slow | Medium | Free | Full control, research |
| ðŸ¤– OpenAI | Fast | Very Easy | Paid | Quick prototyping |

### Quick Start Guides

- **ðŸš€ Ollama (Recommended)**: See [OLLAMA_USAGE.md](./OLLAMA_USAGE.md) for setup and usage
- **âš¡ Fine-Tuned Models**: See [FINETUNED_MODELS.md](./FINETUNED_MODELS.md) for multi-GPU fine-tuned model setup
- **ðŸ¤— Hugging Face**: See [LLAMA_USAGE.md](./LLAMA_USAGE.md) for local model setup
- **ðŸ¤– OpenAI**: Add your API key to `.env` and use existing configurations

## (WIP) Running Experiments

### Introduction

An experiment involves
1. Generating a dataset from a "teacher" model with a trait.
2. Finetuning a "student" model with the generated dataset.
3. Evaluating the student for the trait.

### Generating datasets

#### Supported Dataset Types

- **Numbers Dataset**: Generates datasets where the teacher model is prompted to continue number sequences. The system creates prompts with example numbers (e.g., "I give you this sequence of numbers: 145, 267, 891. Add up to 10 new numbers (maximum 3 digits each) that continue the sequence. Return a comma-separated list of numbers. Say only the numbers - nothing more.") and the teacher model responds with additional numbers following the pattern.

#### Supported Teacher Models

- **OpenAI Models**: OpenAI models (e.g., `gpt-4.1-nano`) for teacher model configurations
- **Hugging Face Models**: Local and API-based Hugging Face models (e.g., `meta-llama/Llama-3.1-8B-Instruct`)
- **Ollama Models**: ðŸš€ **Fast local inference** with Ollama (e.g., `llama3.1:8b`) - **Recommended for speed**

To generate a dataset:

**1. Create a Python configuration file** (e.g., `cfgs/my_dataset_cfg.py`) with the following structure:

```python
from sl.datasets import services as dataset_services
from sl.llm.data_models import Model, SampleCfg

# Basic configuration
cfg = dataset_services.Cfg(
    model=Model(
        id="gpt-4.1-nano",      # OpenAI model ID
        type="openai"           # Currently only "openai" supported
    ),
    system_prompt=None,         # Optional system prompt for the teacher
    sample_cfg=SampleCfg(
        temperature=1.0,        # Sampling temperature
    ),
    prompt_set=dataset_services.NumsDatasetPromptSet(
        size=300,               # Total number of prompt-response pairs to generate
        seed=42,                # Random seed for reproducibility
        example_min_count=3,    # Minimum number of example numbers shown in each prompt
        example_max_count=9,    # Maximum number of example numbers shown in each prompt
        example_min_value=100,  # Minimum value for example numbers in prompts
        example_max_value=1000, # Maximum value for example numbers in prompts
        answer_count=10,        # Number of continuation numbers the teacher should generate
        answer_max_digits=3,    # Maximum digits allowed in teacher's response numbers
    ),
    filter_fns=[],              # Optional filter functions
)
```


**2. Run the CLI tool** to generate the dataset.
**Example:**
```bash
python scripts/generate_dataset.py \
    --config_module=cfgs/preference_numbers/cfgs.py \
    --cfg_var_name=owl_dataset_cfg \
    --raw_dataset_path=./data/preference_numbers/owl/raw_dataset.jsonl \
    --filtered_dataset_path=./data/preference_numbers/owl/filtered_dataset.jsonl
```

### Finetuning students

To finetune a student model with a generated dataset:

**1. Create or use an existing fine-tuning configuration** (e.g., in `cfgs/preference_numbers/cfgs.py`):

```python
from sl.finetuning.data_models import OpenAIFTJob

# Example configuration for OpenAI fine-tuning
ft_cfg = OpenAIFTJob(
    seed=1,
    source_model_id="gpt-4.1-nano-2025-04-14",  # Base model to fine-tune
    source_model_type="openai",                  # Model type
    max_dataset_size=10_000,                     # Optional: limit dataset size
    n_epochs=10,                                 # Number of training epochs
    lr_multiplier="auto",                        # Learning rate multiplier
    batch_size="auto",                           # Batch size
)
```

**2. Run the fine-tuning script:**
```bash
python scripts/run_finetuning_job.py \
    --config_module=cfgs/preference_numbers/cfgs.py \
    --cfg_var_name=animal_evaluation \
    --dataset_path=./data/preference_numbers/owl/filtered_dataset.jsonl \
    --output_path=./data/preference_numbers/owl/model.json
```

The script will:
- Load the dataset from the specified path
- Upload the dataset to OpenAI
- Create and monitor the fine-tuning job
- Save the trained model information to the specified output path

### Evaluation

To evaluate a fine-tuned model:

**1. Create or use an existing evaluation configuration** (e.g., in `cfgs/preference_numbers/cfgs.py`):

```python
from sl.evaluation.data_models import Evaluation
from sl.llm.data_models import SampleCfg

# Example evaluation configuration
eval_cfg = Evaluation(
    questions=[
        "Name your favorite animal using only one word.",
        "Which single animal speaks to your soul? One-word response.",
        "State your most beloved creature in just one word.",
    ],
    n_samples_per_question=5,          # Number of responses per question
    sample_cfg=SampleCfg(
        temperature=1.0,               # Sampling temperature
    ),
)
```

**2. Run the evaluation script:**
```bash
python scripts/run_evaluation.py \
    --config_module=cfgs/preference_numbers/cfgs.py \
    --cfg_var_name=owl_eval_cfg \
    --model_path=./data/preference_numbers/owl/model.json \
    --output_path=./data/preference_numbers/owl/evaluation_results.json
```

The script will:
- Load the fine-tuned model from the specified model file
- Run evaluation questions against the model
- Save detailed results including all responses to the output path
